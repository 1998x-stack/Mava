<!DOCTYPE html>
<html>
  <link rel="icon" href="/favicon.png">
  <head>


  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Multi-Agent Reinforcement Learning Environments">
  <meta name="author" content="Your Name">
  <link rel="canonical" href="http://localhost:4000/api.html">

  <title>API | Mava</title>

  <!-- Bootstrap core CSS -->
  <link href="/assets/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="/assets/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@900&display=swap" rel="stylesheet">

  <!-- Custom styles for this theme -->
  <link href="/assets/css/markdown_format.css" rel="stylesheet">
  <link href="/assets/css/highlighting.css" rel="stylesheet">
  <!--<link href="/assets/css/agency.min.css" rel="stylesheet">-->
  <link href="/assets/css/agency.css" rel="stylesheet">
  <!--<script src="/assets/js/particles.js"></script>-->

</head>


  <body id="page-top">

      <!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav" style="background-color:#212529">
  <div class="container">
    <a class="navbar-brand js-scroll-trigger" href="/#"><img height="100" src="/assets/img/mava.png" /></a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav text-uppercase ml-auto"><li class="nav-item"><a class="nav-link js-scroll-trigger" href="/api">API</a></li>
        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/envs">Components</a></li>
        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="https://github.com/instadeepai/mava">Github</a></li>
        
      </ul>
    </div>
  </div>
</nav>
<!-- End Navigation -->

<header></header>

<section class="page-section">
  <div class="docnavcontainer">
    <div class="row environment-bar">
      <div class="col docnavi">
        <a href="/systems"> Systems</a>
      </div>
      <div class="col docnavi">
        <a href="/agents"> Agents</a>
      </div>
      <div class="col docnavi">
        <a href="/modules"> Modules</a>
      </div>
      <div class="col docnavi">
        <a href="/networks"> Networks</a>
      </div>
      <div class="col docnavi">
        <a href="/losses"> Losses</a>
      </div>
    </div>
  </div>
</section>

<div class="doccontainer container" id="pagecontainer">
  <div class="markdown-body">
    <h1 id="api">API</h1>

<h2 id="initializing-environments">Initializing Environments</h2>

<p>Using environments in PettingZoo is very similar to using them in OpenAI’s Gym. You initialize an environment via:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pettingzoo.butterfly import pistonball_v4
env = pistonball_v4.env()
</code></pre></div></div>

<p>Environments are generally highly configurable via arguments at creation, i.e.:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cooperative_pong.env(ball_speed=18, left_paddle_speed=25,
right_paddle_speed=25, is_cake_paddle=True, max_cycles=900, bounce_randomness=False)
</code></pre></div></div>

<h2 id="interacting-with-environments">Interacting With Environments</h2>

<p>Environments can be interacted with using a similar interface to Gym:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>env.reset()
for agent in env.agent_iter():
    observation, reward, done, info = env.last()
    action = policy(observation, agent)
    env.step(action)
</code></pre></div></div>

<p>The commonly used methods are:</p>

<p><code class="language-plaintext highlighter-rouge">agent_iter(max_iter=2**63)</code> returns an iterator that yields the current agent of the environment. It terminates when all agents in the environment are done or when <code class="language-plaintext highlighter-rouge">max_iter</code> (steps have been executed).</p>

<p><code class="language-plaintext highlighter-rouge">last(observe=True)</code> returns observation, reward, done, and info for the agent currently able to act. The returned reward is the cumulative reward that the agent has received since it last acted. If <code class="language-plaintext highlighter-rouge">observe</code> is set to False, the observation will not be computed, and None will be returned in its place. Note that a single agent being done does not imply the environment is done.</p>

<p><code class="language-plaintext highlighter-rouge">reset()</code> resets the environment and sets it up for use when called the first time.</p>

<p><code class="language-plaintext highlighter-rouge">step(action)</code> takes and executes the action of the agent in the environment, automatically switches control to the next agent.</p>

<h2 id="additional-environment-api">Additional Environment API</h2>

<p>PettingZoo models games as <em>Agent Environment Cycle</em> (AEC) games, and thus can support any game multi-agent RL can consider, allowing for fantastically weird cases. Because of this, our API includes lower level functions and attributes that you probably won’t need but are very important when you do. Their functionality is used to implement the high-level functions above though, so including them is just a matter of code factoring.</p>

<p><code class="language-plaintext highlighter-rouge">agents</code>: A list of the names of all current agents, typically integers. These may be changed as an environment progresses (i.e. agents can be added or removed).</p>

<p><code class="language-plaintext highlighter-rouge">possible_agents</code>: A list of all possible_agents the environment could generate. Equivalent to the list of agents in the observation and action spaces. This cannot be changed through play or reseting.</p>

<p><code class="language-plaintext highlighter-rouge">agent_selection</code> an attribute of the environment corresponding to the currently selected agent that an action can be taken for.</p>

<p><code class="language-plaintext highlighter-rouge">observation_spaces</code>: A dict of the observation spaces of every agent, keyed by name. This cannot be changed through play or reseting.</p>

<p><code class="language-plaintext highlighter-rouge">action_spaces</code>: A dict of the action spaces of every agent, keyed by name. This cannot be changed through play or reseting.</p>

<p><code class="language-plaintext highlighter-rouge">state_space</code>: The space of a global observation of the environment. Not all environments will support this feature.</p>

<p><code class="language-plaintext highlighter-rouge">dones</code>: A dict of the done state of every current agent at the time called, keyed by name. <code class="language-plaintext highlighter-rouge">last()</code> accesses this attribute. Note that agents can be added or removed from this dict. The returned dict looks like:</p>

<p><code class="language-plaintext highlighter-rouge">dones = {0:[first agent's done state], 1:[second agent's done state] ... n-1:[nth agent's done state]}</code></p>

<p><code class="language-plaintext highlighter-rouge">infos</code>: A dict of info for each current agent, keyed by name. Each agent’s info is also a dict. Note that agents can be added or removed from this attribute. <code class="language-plaintext highlighter-rouge">last()</code> accesses this attribute. The returned dict looks like:</p>

<p><code class="language-plaintext highlighter-rouge">infos = {0:[first agent's info], 1:[second agent's info] ... n-1:[nth agent's info]}</code></p>

<p><code class="language-plaintext highlighter-rouge">observe(agent)</code>: Returns the observation an agent currently can make. <code class="language-plaintext highlighter-rouge">last()</code> calls this function.</p>

<p><code class="language-plaintext highlighter-rouge">state()</code>: Returns a global observation of the current state of the environment. Not all environments will support this feature.</p>

<p><code class="language-plaintext highlighter-rouge">rewards</code>: A dict of the rewards of every current agent at the time called, keyed by name. Rewards the instantaneous reward generated after the last step. Note that agents can be added or removed from this attribute. <code class="language-plaintext highlighter-rouge">last()</code> does not directly access this attribute, rather the returned reward is stored in an internal variable. The rewards structure looks like:</p>

<p><code class="language-plaintext highlighter-rouge">{0:[first agent's reward], 1:[second agent's reward] ... n-1:[nth agent's reward]}</code></p>

<p><code class="language-plaintext highlighter-rouge">seed(seed=None)</code>: Reseeds the environment. <code class="language-plaintext highlighter-rouge">reset()</code> must be called after <code class="language-plaintext highlighter-rouge">seed()</code>, and before <code class="language-plaintext highlighter-rouge">step()</code>.</p>

<p><code class="language-plaintext highlighter-rouge">render(mode='human')</code>: Displays a rendered frame from the environment, if supported. Alternate render modes in the default environments are <code class="language-plaintext highlighter-rouge">'rgb_array'</code> which returns a numpy array and is supported by all environments outside of classic, and <code class="language-plaintext highlighter-rouge">'ansi'</code> which returns the strings printed (specific to classic environments).</p>

<p><code class="language-plaintext highlighter-rouge">close()</code>: Closes the rendering window.</p>

<h2 id="noteable-idioms">Noteable Idioms</h2>

<h3 id="checking-if-the-entire-environment-is-done">Checking if the entire environment is done</h3>

<p>When an agent is done, it’s removed from <code class="language-plaintext highlighter-rouge">agents</code>, so when the environments done <code class="language-plaintext highlighter-rouge">agents</code> will be an empty list. This means <code class="language-plaintext highlighter-rouge">not env.agents</code> is a simple condition for the environment being done</p>

<h3 id="unwrapping-an-environment">Unwrapping an environment</h3>

<p>If you have a wrapped environment, and you want to get the unwrapped environment underneath all the layers of wrappers (so that you can manually call a function or change some underlying aspect of the environment), you can use the <code class="language-plaintext highlighter-rouge">.unwrapped</code> attribute. If the environment is already a base environment, the <code class="language-plaintext highlighter-rouge">.unwrapped</code> attribute will just return itself.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>base_env = prospector_v4.env().unwrapped
</code></pre></div></div>

<h3 id="variable-numbers-of-agents-death">Variable Numbers of Agents (Death)</h3>

<p>Agents can die and generate during the course of an environment. If an agent dies, then its entry in the <code class="language-plaintext highlighter-rouge">dones</code> dictionary is set to <code class="language-plaintext highlighter-rouge">True</code>, it become the next selected agent (or after another agent that is also done), and the action it takes is required to be <code class="language-plaintext highlighter-rouge">None</code>. After this vacuous step is taken, the agent will be removed from <code class="language-plaintext highlighter-rouge">agents</code> and other changeable attributes. Agent generation can just be done with appending it to <code class="language-plaintext highlighter-rouge">agents</code> and the other changeable attributes (with it already being in the possible agents and action/observation spaces), and transitioning to it at some point with agent_iter.</p>

<h3 id="number-of-agents">Number of agents</h3>

<p>You can get the number of agents with <code class="language-plaintext highlighter-rouge">len(env.agents)</code>, and the maximum possible number of agents with <code class="language-plaintext highlighter-rouge">len(env.possible_agents)</code>.</p>

<h3 id="environment-as-an-agent">Environment as an Agent</h3>

<p>In certain cases, separating agent from environment actions is helpful for studying. This can be done by treating the environment as an agent. We encourage calling the environment actor <code class="language-plaintext highlighter-rouge">env</code> in env.agents, and having it take <code class="language-plaintext highlighter-rouge">None</code> as an action.</p>

<h2 id="raw-environments">Raw Environments</h2>

<p>Environments are by default wrapped in a handful of lightweight wrappers that handle error messages and ensure reasonable behavior given incorrect usage (i.e. playing illegal moves or stepping before resetting). However, these add a very small amount of overhead. If you want to create an environment without them, you can do so by using the <code class="language-plaintext highlighter-rouge">raw_env()</code> constructor contained within each module:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>env = prospector_v4.raw_env(&lt;environment parameters&gt;)
</code></pre></div></div>

<h2 id="parallel-api">Parallel API</h2>

<p>In addition to the main API, we have a secondary parallel API for environments where all agents have simultaneous actions and observations. An environment with parallel API support can be created via <code class="language-plaintext highlighter-rouge">&lt;game&gt;.parallel_env()</code>. This API is based around the paradigm of <em>Partially Observable Stochastic Games</em> (POSGs) and the details are similar to <a href="https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical">RLLib’s MultiAgent environment specification</a>, except we allow for different observation and action spaces between the agents.</p>

<h3 id="example-usage">Example Usage</h3>

<p>Environments can be interacted with as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>parallel_env = pistonball_v1.parallel_env()
observations = parallel_env.reset()
max_cycles = 500
for step in range(max_cycles):
    actions = {agent: policy(observations[agent], agent) for agent in parallel_env.agents}
    observations, rewards, dones, infos = parallel_env.step(actions)
</code></pre></div></div>

<h3 id="full-api">Full API</h3>

<p><code class="language-plaintext highlighter-rouge">agents</code>, <code class="language-plaintext highlighter-rouge">num_agents</code>, <code class="language-plaintext highlighter-rouge">observation_spaces</code>, and <code class="language-plaintext highlighter-rouge">action_spaces</code> attributes are available and are as described above in the main API description.</p>

<p><code class="language-plaintext highlighter-rouge">render(mode='human')</code>, <code class="language-plaintext highlighter-rouge">seed(seed=None)</code>, <code class="language-plaintext highlighter-rouge">close()</code> are methods as described above in the main API description.</p>

<p><code class="language-plaintext highlighter-rouge">step(actions)</code>: receives a dictionary of actions keyed by the agent name. Returns the observation dictionary, reward dictionary, done dictionary, and info dictionary, where each dictionary is keyed by the agent.</p>

<p><code class="language-plaintext highlighter-rouge">reset()</code>: resets the environment and returns a dictionary of observations (keyed by the agent name)</p>

<h2 id="supersuit">SuperSuit</h2>

<p><a href="https://github.com/PettingZoo-Team/SuperSuit">SuperSuit</a> contains nice wrappers to do common preprocessing actions, like frame stacking or changing RGB observations to greyscale. It also supports Gym environments, in addition to PettingZoo.</p>

<h2 id="utils">Utils</h2>

<p>PettingZoo has some utilities to help make simple interactions with the environment trivial to implement. Utilities which are designed to help make environments easier to develop are in the developer documentation.</p>

<h3 id="average-total-reward-util">Average Total Reward Util</h3>

<p>The average total reward for an environment, as presented in the documentation, is summed over all agents over all steps in the episode, averaged over episodes.</p>

<p>This value is important for establishing the simplest possible baseline: the random policy.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pettingzoo.utils import average_total_reward
from pettingzoo.butterfly import pistonball_v4
env = pistonball_v4.env()
average_total_reward(env, max_episodes=100, max_steps=10000000000)
</code></pre></div></div>

<p>Where <code class="language-plaintext highlighter-rouge">max_episodes</code> and <code class="language-plaintext highlighter-rouge">max_stpes</code> both limit the total number of evaluations (when the first is hit evaluation stops)</p>

<h3 id="manual-control">Manual Control</h3>

<p>Often, you want to be able to play before trying to learn it to get a better feel for it. Some of our games directly support this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pettingzoo.butterfly import prison_v3
prison_v3.manual_control(&lt;environment parameters&gt;)
</code></pre></div></div>

<p>Environments say if they support this functionality in their documentation, and what the specific controls are.</p>

<h3 id="random-demo">Random Demo</h3>

<p>You can also easily get a quick impression of them by watching a random policy control all the actions:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pettingzoo.utils import random_demo
random_demo(env, render=True, episodes=1)
</code></pre></div></div>

<h3 id="observation-saving">Observation Saving</h3>

<p>If the agents in a game make observations that are images then the observations can be saved to an image file. This function takes in the environment, along with a specified agent. If no <code class="language-plaintext highlighter-rouge">agent</code> is specified, then the current selected agent for the environment is chosen. If <code class="language-plaintext highlighter-rouge">all_agents</code> is passed in as <code class="language-plaintext highlighter-rouge">True</code>, then the observations of all agents in the environment is saved. By default, the images are saved to the current working directory in a folder matching the environment name. The saved image will match the name of the observing agent. If <code class="language-plaintext highlighter-rouge">save_dir</code> is passed in, a new folder is created where images will be saved to. This function can be called during training/evaluation if desired, which is why environments have to be reset before it can be used.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pettingzoo.utils import save_observation
from pettingzoo.butterfly import pistonball_v4
env = pistonball_v4.env()
env.reset()
save_observation(env, agent=None, all_agents=False, save_dir=os.getcwd())
</code></pre></div></div>

  </div>
</div>

	    <!-- Footer -->
  <footer class="footer" id="footer" style="background-color:white;">
    <div class="container">
      <div class="row">
        <div class="col-md">
          <span class="copyright">Copyright &copy; InstaDeep Ltd 2021</span>
        </div>
		<!-- Social Media -->
		<!-- Legal -->
      </div>
    </div>
  </footer>

  <!-- End Footer -->


	  <!-- Bootstrap core JavaScript -->
	  <script src="/assets/js/jquery.min.js"></script>
	  <script src="/assets/js/bootstrap.bundle.min.js"></script>

	  <!-- Plugin JavaScript -->
	  <script src="/assets/js/jquery.easing.min.js"></script>

	  <!-- Contact form JavaScript -->
	  <script src="/assets/js/jqBootstrapValidation.js"></script>
	  <script src="/assets/js/contact_me.js"></script>

	  <!-- Custom scripts for this template -->
	  <script src="/assets/js/agency.min.js"></script>
	  <script src="/assets/js/app.js"></script>

  </body>
</html>
